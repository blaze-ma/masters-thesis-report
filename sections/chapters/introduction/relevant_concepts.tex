\pagebreak

\subsection{Knowledge Graphs}\label{subsec:introduction-knowledge-graphs}

\begin{wrapfigure}{r}{0.5\textwidth} % Float right, width half the text width
    \centering
    \includegraphics[width=\linewidth, scale=0.5]{figures/kg-example}
    \caption{Example of a knowledge graph.}
    \label{fig:kg-example}
\end{wrapfigure}
One may think of Knowledge Graphs as directed heterogeneous graphs
created with the intent of representing knowledge bases in a machine interpretable manner.
Nodes may (but not bound to) be objects, events, situations, abstract concepts or locations,
with the edges between the nodes representing conceptual relationships among the entities.

Knowledge graphs are often represented as lists of statements such that a statement is: $s = (h,r,t)$
where $h$ refers to the head entity, $t$ represents the tail entity and $r$ represents the
edge connecting the two entities.
These statements have also been called triplets.

Knowledge graphs are often backed by predefined ontologies.
Ontologies define entities and relationships referenced in the list of statements.
serving as their explicit schema, making the parsing of and
work with these graphs easier.
One may also think of knowledge graphs as the combination of the statements, and the relevant ontologies.

Some use cases of knowledge graphs include Google's enhanced contextual response to search queries~\cite{GoogleKnowledgeGraph} (using their internal
knowledge graph) or a more recent example: researchers have been experimenting with augmenting large language models
with knowledge graphs~\cite{LLMKG}, in order to ensure factual answers.

Examples of publicly available knowledge graphs are: Wikidata~\cite{Wikidata} a generic
knowledge graph backing the rest of the Wikimedia ecosystem, or WN18 dataset parsed from WordNet
introduced by~\cite{TransE} and is commonly used to evaluate the performance of various graph machine learning models.



\FloatBarrier
\subsection{Graph Representation Learning}\label{subsec:introduction-graph-representation-learning}
Graph Representation Learning is a research field dedicated to creating various methods of embedding nodes of a graph into
a low-dimensional vector space that may be used to perform various downstream tasks such as graph and node classification or link prediction.

These GRL models rely on an encoder-decoder model.
"The intuition behind the encoder-decoder idea is the following:
If we can learn to decode high-dimensional graph information—such as the global positions of
nodes in the graph or the structure of local graph neighborhoods—from encoded low-dimensional embeddings, then, in principle,
these embeddings should contain all information necessary for downstream machine learning tasks"~\cite{RLGMandA}

In other words, while downstream tasks consume the vector representation, the decoder is used to create a well-trained encoder model.

The encoder may create either shallow embeddings or deep embeddings.
Shallow embedding techniques are generally simpler and faster to train but
may struggle to capture highly complex patterns and hierarchical relationships within the graph.

Example of shallow embedding methods include: Node2Vec~\cite{Node2vec} or DeepWalk~\cite{DeepWalk}

Deep embedding methods are commonly some variation of Graph Neural Networks detailed in section~\ref{subsec:introduction-graph-neural-networks}

\subsection{Graph Neural Networks}\label{subsec:introduction-graph-neural-networks}
The challenge in creating encoding models that capture deep insight into graph structures is that graph structures are inherently variable ~\cite{GRLBook}.

For example, if one was to build a model that categorizes social networks using classical tools such as Convolutional Neural Networks or Recurring Neural Networks,
The model would be restricted to graphs with a set number of nodes.

Graph neural networks, which use the concept of neural message passing, are used to address the above issue and leverage the structure of graphs.
The recent state-of-the-art knowledge graph completion models are commonly based on some neural message passing framework~\cite{LPSOTA} due to their inherent ability to capture
deeper neighborhood structures.

Figure~\ref{fig:gnn} illustrates the basic concept of message passing.
In a message-passing layer, each neighboring node exchanges its features and encodings with others.
For each node, these incoming messages are aggregated to form a new encoding for that node.
Depending on the architecture, several message-passing layers may be stacked.
By the end of this process, nodes will represent a combination of their initial features and the features of their local neighborhood.

\begin{figure}[h] % [h] attempts to place figure here, other options like [t]op, [b]ottom
    \centering % Centers the figure horizontally
    \includegraphics[width=0.9\linewidth]{figures/gnn} % Include the image with desired width
    \caption{A figure from the book Graph Representation Learning~\cite{GRLBook} showcases the general intuition behind neural message passing.} % Add a caption
    \label{fig:gnn} % Assign a label for referencing the figure in text
\end{figure}


\subsection{Wikidata}\label{subsec:introduction-wikidata}
"Wikidata is a free, collaborative, multilingual, secondary knowledge base, collecting structured data to provide support for Wikipedia, Wikimedia Commons, the other wikis of the Wikimedia movement,
and to anyone in the world."

"The Wikidata repository consists mainly of items, each one having a label, a description and any number of aliases.
Items are uniquely identified by a Q followed by a number, such as Douglas Adams (Q42).
Statements describe detailed characteristics of an Item and consist of a property and a value.
Properties in Wikidata have a P followed by a number, such as with educated at (P69)."~\cite{Wikidata}

As detailed later in section~\ref{sec:graph-building}, the initial dataset that forms the basis of this thesis contains some nodes that have inferred Wikidata IDs.
Therefore, Wikidata will be a potential source for enriching the dataset and creating a denser network.

Moreover, a secondary object of this thesis is to represent Kitāb Mu'jam al-Buldān in a consumable knowledge graph format.
Therefore, it is easy to argue that using Wikidata's ontology will streamline such efforts.


\subsection{Evaluation Metrics}
This section details the commonly used metrics in link prediction literature.
The usual environment for model evaluation involves the generation of synthetic negative triplets.
The need for negative is explained in subsection~\ref{subsec:negatives-generation}
These negative triplets, alongside the positives are then ranked based on some score

Notationally, in the next few sections $Q$ refers to a set of triples $s = (h,r,t)$ where $s \in Q$~\cite{ampligraph}
\subsubsection{Rank Score}
Simple integer value, the rank of a triplet in a vector of triplets.

\subsubsection{Mean Rank}
The average rank score of a vector of positive triplets in $Q$
\[
MR = \frac{1}{|Q|}\sum_{i = 1}^{|Q|}rank_{(h, r, t)_i}
\]

\subsubsection{Mean Reciprocal Rank}
Similar to the mean rank, but instead of averaging triplet ranks, the reciprocal ranks are averaged.
This should make the metric more robust against outliers~\cite{ampligraph}.

\[
    MRR = \frac{1}{|Q|}\sum_{i = 1}^{|Q|}\frac{1}{rank_{(h, r, t)_i}}
\]

\subsubsection{Hits@K}
The ratio of how many positive triplets make it to the top $K$ list of triplets when ranked against negative triplets.
\[
    Hits@K = \sum_{i = 1}^{|Q|} 1 \, \text{if } rank_{(h, r, t)_i} \leq K
\]
In this thesis $K={1,3,5}$