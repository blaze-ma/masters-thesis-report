In the initial KGE experiments, the dataset generated in section~\ref{sec:building-complete-knowledge-graph} was used as data for the models.
The dataset was split into train, test and validation dataset in a $80\%$, $10\%$ and $10\%$ ratio.
The split was done in a transductive manner, such that there are no, previously unseen nodes in the test and validation
dataset.

The implementation of the models were done using the Python libraryâ€”Ampligraph~\cite{ampligraph}

To find the best performing model and the most optimal hyperparameters, a grid search was performed, with the MRR score
being the basis of comparison.

For each method detailed in section~\ref{sec:embedding-methods}, the following hyperparameters were tuned:
\begin{itemize}
    \item \textbf{Batch size:} 128, 256, 512, 1024, 2048
    \item \textbf{Epochs:} 5, 25, 50, 100, 200, 250
    \item \textbf{Eta:} 5, 10, 15 (\textit{eta specifies the number of corruptions to generate per each positive})
    \item \textbf{Vector embedding size:} 50, 100, 150, 200
    \item \textbf{Loss function:} pairwise, nll
    \item \textbf{Optimizer:} AdaGrad, adam
\end{itemize}

The best performing model and hyperparameter combinations are shown on table~\ref{tab:kge-params}.

\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|l|l|l|}
        \hline
        & \textbf{batch size} & \textbf{epochs} & \textbf{k} & \textbf{eta} & \textbf{loss} & \textbf{optimizer} \\ \hline
        \textbf{TransE} & 2048 & 200 & 150 & 5 & pairwise & adam \\ \hline
        \textbf{RotatE} & 1024 & 90 & 150 & 15 & nll & adam \\ \hline
        \textbf{HolE} & 128 & 80 & 200 & 5 & pairwise & adam \\ \hline
        \textbf{DistMult} & 128 & 60 & 200 & 5 & pairwise & adam \\ \hline
        \textbf{ComplEx} & 128 & 50 & 200 & 5 & pairwise & adam \\ \hline
    \end{tabular}
    \caption{Best performing KGE hyperparameters}
    \label{tab:kge-params}
\end{table}